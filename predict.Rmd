---
title: "Practical Data Science - Prediction Assignment"
output: html_document
---

## Introduction

In this project, we will use a data set from accelerometers on the belt, forearm, arm and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 diferent ways. 
The goal is to use the provided training cases to build a model to predict the manner in which 20 test cases did the exercise. 

## Load necessary packages
```{r, message=FALSE, warning=FALSE}
library(caret)
library(dplyr)        # For data manipulation functions
library(ggplot2)      # For graphics
library(randomForest) # For random forest
```

## Load the data

### Create function to load data
```{r}
load_data <- function(file){
  # Build a path for the data file name
  fn <- file.path('data', file)
  
  df <- read.csv(fn, stringsAsFactor = TRUE, na.strings = c('NA', '')
                  , quote = '\"')
  return (df)
}
```

### Load the data files
```{r, cache=TRUE}
# Load the training data
raw_train_data <- load_data('pml-training.csv')
raw_train_data <- tbl_df(raw_train_data)

# Load the test data
raw_test_data <- load_data('pml-testing.csv')
raw_test_data <- tbl_df(raw_test_data)
```

## Take a first look at the raw data
```{r}
# How many observation and variables we have
d <- dim(raw_train_data)
df1 <- data.frame(Data = 'train',Obs = d[1], Vars = d[2])
d <- dim(raw_test_data)
df2 <- data.frame(Data = 'test',Obs = d[1], Vars = d[2])
rbind(df1, df2)

glimpse(raw_train_data)
```

There are `r df1$Obs` observations and `r df1$Vars` variables in the training data set. The response variable is *classe*.

## Data Preparation

### Look for variables with lots of NAs and remove them

After a quick look at the data, we realize that there are some variables with lots of *NAs* values. So in order to simplify our job, we wil remove the variables with a high percentage of *NAs* values.

```{r}
# Get the percentage of NAs values in each variable
perc_nas <- sapply(raw_train_data, function (x) sum(is.na(x)) / length(x))

# Get a list of variables with more than 90% of NAs values
high_nas <- names(perc_nas[perc_nas > 0.9])

# Remove the hight NAs variables from train data
train_data <- raw_train_data %>%
  select( - one_of(high_nas))

# Remove the hight NAs variables from test data
test_data <- raw_test_data %>%
  select( - one_of(high_nas))
```

### Remove variables that do not add to the model

We will remove the variables we believe do not add to the model.

```{r}
# Create a list of variables to remove
var_list <- c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp'
              , 'X', 'user_name')

# Remove from train data
train_data <- train_data %>%
   select(-one_of(var_list))

# Remove from test data
test_data <- test_data %>%
   select(-one_of(var_list))
```

### Remove zero and near-zero variance variables

We identify the predictors that have near-zero variance and remove them. This kind of predictors may cause problems depending on the type of model chosen.

```{r}
# Get a list of variables with near-zero variance
nzv <- nearZeroVar(train_data)

# Summarize near zero variance variables
summary(train_data[ , nzv])

# Remove from the train data
train_data <- train_data[ , -nzv]

# Remove from the test data
test_data <- test_data[ , -nzv]
```

### Data partition

We will split the train data in two in order to have a data slice to train the model and another data slice to validate the model. Besides, we will be able to estimate the out-of-sample error rate.

```{r}
ind <- createDataPartition(y = train_data$classe
                           , p = 0.6
                           , list = FALSE)
train_slice <- train_data[ind,]
valid_slice <- train_data[-ind,]

dim(train_slice)
dim(valid_slice)
```

## Build a Random Forest model

We will start with a simple tree model. In order to estimate the out of sample error rate, we will be using k-fold cross-validation as the resampling method. 

### Cross-validation

By default, the train function of the Caret package uses the bootstrap for resampling. We will switch to k-fold cross-validation instead.
We can change the type of resampling via the *trainControl* function.

### Training

Train a random forest model, using the parameters defined above. The *tuneLength* parameter is used to specify the number of models to evaluate. The best one will be chosen by the function as the final one.

```{r}
# Set the seed to generate the same random numbers always.
set.seed(123)
# 
# Generate parameters to be used by the train function
control <- trainControl(method = 'cv' # K-fold cross-validation
                        , number = 2  # Number of folds to use
                        )
# Start the clock
ptm <- proc.time()

rf_model <- train (
                    classe ~ .
                    , data = train_data
                    , method = 'rf'       # Random forest
                    , trControl = control
                    , tuneLength = 1      # Number of models to be evaluated
                    , metric = 'Accuracy'
                    )

# Stop the clock
ptm <- proc.time() - ptm

rf_model
```

Elapsed training time: `r sprintf('%.2f seconds',as.numeric(ptm['elapsed']))`

### Estimation of the out of sample error

From the results above, we estimate that the out of sample error rate of the model when applied to the test data is: , which is the average percentage of observation that were incorrectly classified in the cross validation process.

## Predict the response variable for the test data
```{r}
#pred <- predict(rf_model, test_data[, predictors] )
```

