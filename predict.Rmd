---
title: "Practical Data Science - Prediction Assignment"
output: html_document
---

## Introduction

This document describes the 

## Load necessary packages
```{r, message=FALSE, warning=FALSE}
library(dplyr)      # For data manipulation functions
library(caret)      # For classification and regression training
library(rpart.plot) # For the fancyRpartPlot function
library(rattle)     # For prettier tree plots
library(ggplot2)    # For graphics
```


## Create function to load data
```{r}
load_data <- function(file){
  # Build a path for the data file name
  fn <- file.path('data', file)
  
  df <- read.csv(fn, stringsAsFactor = TRUE, na.strings = c('NA', '')
                  , quote = '\"')
  return (df)
}
```

## Load the data files
```{r, cache=TRUE}
###############################################################################
# Load the data
###############################################################################
# Load the training data
raw_train_data <- load_data('pml-training.csv')
raw_train_data <- tbl_df(raw_train_data)

# Load the test data
raw_test_data <- load_data('pml-testing.csv')
raw_test_data <- tbl_df(raw_test_data)
```

## Take a first look at the raw data
```{r}
###############################################################################
# First look at the raw data
###############################################################################
# How many observation and variables we have
d <- dim(raw_train_data)
df1 <- data.frame(Data = 'train',Obs = d[1], Vars = d[2])
d <- dim(raw_test_data)
df2 <- data.frame(Data = 'test',Obs = d[1], Vars = d[2])
rbind(df1, df2)

glimpse(raw_train_data)
```

There are `r df1$Obs` observations and `r df1$Vars` variables in the training data set. The response variable is *classe*.

## Look for variables with lots of NAs
```{r}
###############################################################################
# Look for variables with lots of NAs
###############################################################################
# Get the percentage of NAs values in each variable
perc_nas <- sapply(raw_train_data, function (x) sum(is.na(x)) / length(x))

# Get a list of variables with more than 90% of NAs values
high_nas <- names(perc_nas[perc_nas > 0.9])

high_nas
```

## Remove variables with lots of NAs
```{r}
###############################################################################
# Remove variables with lots of NAs
###############################################################################

# Remove the hight NAs variables from train data
train_data <- raw_train_data %>%
  select( - one_of(high_nas))

# Remove the hight NAs variables from test data
test_data <- raw_test_data %>%
  select( - one_of(high_nas))

```

## Remove timestamp variables
```{r}
###############################################################################
# Remove timestamp variables
###############################################################################
# Create a list of variables to remove
var_list <- c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')

# Remove from train data
train_data <- train_data %>%
   select(-one_of(var_list))

# Remove from test data
test_data <- test_data %>%
   select(-one_of(var_list))
```

## Remove zero and near zero-variance variables
```{r}
###############################################################################
# Remove zero and near zero-variance variables
###############################################################################
nzv <- nearZeroVar(train_data)

dim(train_data)
train_data <- train_data %>%
  select(-nzv)
dim(train_data)

dim(test_data)
test_data <- test_data %>%
  select(-nzv)
dim(test_data)
```

## Create a list of names of predictors variables
```{r}
###############################################################################
# Create list of names of predictors variables
###############################################################################
# Create list of numeric variables 
is_num <- sapply(train_data, is.numeric)
numerics <- names(is_num[is_num == TRUE])

# Remove the X variable as it is a kind of key that is unique for each
# observation
predictors <- numerics[numerics != 'X']

```

## Let us start with a Tree model
```{r}
###############################################################################
# Build a tree model
###############################################################################
set.seed(123)

control <- trainControl(method = 'cv'   # Cross-validation
                        , number = 5   # Number of subsamples to take
                        )

tree_model <- train(
                    x = train_data[, predictors] # Predictors
                    , y = train_data$classe      # Outcome variable
                    , method = 'rpart'
                    , trControl = control
                    , tuneLength = 10 # Number of models to be evaluated
                    , metric = 'Accuracy'
                    )

# Get the accuracy from the resampling results for the chosen model
acc <-getTrainPerf(tree_model)[1]

# Plot the tree
fancyRpartPlot(tree_model$finalModel, main = '', sub = '')
```

### Estimation of the out of sample error

From the results above, we estimate that the out of sample error is: `r sprintf('%.2f%%', (1 - acc) * 100)`, which is the average percentage of observation that were incorrectly classified in the cross validation process.
  
## Plot the variable at the root of the tree  
The variable at the root of tree is the one that best separates the outcomes.

```{r}
train_data %>%
  ggplot(aes(x = roll_belt , colour = classe)) +
    geom_line(stat = 'density')

```

## 
