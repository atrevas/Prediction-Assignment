---
title: "Practical Data Science - Prediction Assignment"
output: html_document
---

## Introduction

In this project, we will use a data set from accelerometers on the belt, forearm, arm and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 diferent ways. 
The goal is to use the provided training cases to build a model to predict the manner in which 20 test cases did the exercise. 

## Load necessary packages
```{r, message=FALSE, warning=FALSE}
library(caret)
library(dplyr)        # For data manipulation functions
library(ggplot2)      # For graphics
library(randomForest) # For random forest
```

## Load the data

### Create function to load data
```{r}
load_data <- function(file){
  # Build a path for the data file name
  fn <- file.path('data', file)
  
  df <- read.csv(fn, stringsAsFactor = TRUE, na.strings = c('NA', '')
                  , quote = '\"')
  return (df)
}
```

### Load the data files
```{r, cache=TRUE}
###############################################################################
# Load the data
###############################################################################
# Load the training data
raw_train_data <- load_data('pml-training.csv')
raw_train_data <- tbl_df(raw_train_data)

# Load the test data
raw_test_data <- load_data('pml-testing.csv')
raw_test_data <- tbl_df(raw_test_data)
```

## Take a first look at the raw data
```{r}
###############################################################################
# First look at the raw data
###############################################################################
# How many observation and variables we have
d <- dim(raw_train_data)
df1 <- data.frame(Data = 'train',Obs = d[1], Vars = d[2])
d <- dim(raw_test_data)
df2 <- data.frame(Data = 'test',Obs = d[1], Vars = d[2])
rbind(df1, df2)

glimpse(raw_train_data)
```

There are `r df1$Obs` observations and `r df1$Vars` variables in the training data set. The response variable is *classe*.

## Data Preparation

### Look for variables with lots of NAs and remove them

After a quick look at the data, we realize that there are some variables with lots of *NAs* values. So in order to simplify our job, we wil remove the variables with a high percentage of *NAs* values.

```{r}
###############################################################################
# Remove variables with lots of NAs
###############################################################################
# Get the percentage of NAs values in each variable
perc_nas <- sapply(raw_train_data, function (x) sum(is.na(x)) / length(x))

# Get a list of variables with more than 90% of NAs values
high_nas <- names(perc_nas[perc_nas > 0.9])

# Remove the hight NAs variables from train data
train_data <- raw_train_data %>%
  select( - one_of(high_nas))

# Remove the hight NAs variables from test data
test_data <- raw_test_data %>%
  select( - one_of(high_nas))
```

### Remove timestamp variables

We will remove the timestamp variables because we believe they do not add to the model.

```{r}
###############################################################################
# Remove timestamp variables
###############################################################################
# Create a list of variables to remove
var_list <- c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')

# Remove from train data
train_data <- train_data %>%
   select(-one_of(var_list))

# Remove from test data
test_data <- test_data %>%
   select(-one_of(var_list))
```

### Remove zero and near zero-variance variables

We identify the predictors that have only a few unique values and remove them. This kind of predictors may cause problems depending on the type of model chosen.

```{r}
###############################################################################
# Remove zero and near zero-variance variables
###############################################################################
nzv <- nearZeroVar(train_data)

# Summarize near zero variance variables
summary(train_data[ , nzv])

# Remove from the train data
train_data <- train_data[ , -nzv]

# Remove from the test data
test_data <- test_data[ , -nzv]
```

## Create a list of names of predictors variables
```{r}
###############################################################################
# Create list of names of predictors variables
###############################################################################
# Create list of numeric variables 
is_num <- sapply(train_data, is.numeric)
numerics <- names(is_num[is_num == TRUE])

# Remove the X variable as it is a kind of key that is unique for each
# observation
predictors <- numerics[numerics != 'X']

```

## First Tree model

We will start with a simple tree model. In order to estimate the out of sample error rate, we will be using k-fold cross-validation as the resampling method. 

### Cross-validation

Set the seed to generate the same random numbers always.
```{r}
set.seed(123)
```

By default, the train function of the Caret package uses the bootstrap for resampling. We will switch to k-fold cross-validation instead.
We can change the type of resampling via the *trainControl* function.
```{r}
# Generate parameters to be used by the train function
control <- trainControl(method = 'cv' # K-fold cross-validation
                        , number = 2  # Number of folds to use
                        )
```
### Training

Train a tree model, using the parameters defined above. The *tuneLength* parameter is used to specify the number of models to evaluate. The best one will be chosen by the function as the final one.

### Estimation of the out of sample error
```{r}
# Get the average accuracy over cross-validation iterations
#acc <-getTrainPerf(tree_model)[1]

# Transform accuracy into an error rate
#err_rate <- as.numeric(1 - acc)
#err_rate
#`r sprintf('%.2f%%', err_rate * 100)`
```

## Build a Random Forest model

We believe it is possible to build a better model. So let us give Random Forest a try.

```{r}
###############################################################################
# Build a random forest model
###############################################################################
# set.seed(123)
# 
# control <- trainControl(method = 'cv'   # Cross-validation
#                         , number = 2  # Number of subsamples to take
#                         )
# 
# rf_model <- train (
#                     x = train_data[, predictors] # Predictors
#                     , y = train_data$classe      # Outcome variable
#                     , method = 'rf'
#                     , trControl = control
#                     , tuneLength = 1 # Number of models to be evaluated
#                     , metric = 'Accuracy'
#                     )
# 
# # Print the tree model
# rf_model
# 
# # Get the average accuracy over cross-validation iterations
# acc <-getTrainPerf(rf_model)[1]
```

### Estimation of the out of sample error

From the results above, we estimate that the out of sample error rate of the model when applied to the test data is: , which is the average percentage of observation that were incorrectly classified in the cross validation process.

### Plot information about the random forest model

```{r}
# Build a data frame with variables importance in the random forest model
# vi <- varImp(rf_model)$importance
# vi_df <- data.frame( var = rownames(vi), imp = vi$Overall)
# 
# # Plot the ranking of variables importance
# vi_df %>% 
#   top_n(10, imp) %>% 
#   ggplot(aes(x = imp, y = reorder(var, imp))) +
#     geom_point(size = 3) +
#     xlab('Importance') +
#     ylab('Variable')
```

## Predict the response variable for the test data
```{r}
#pred <- predict(rf_model, test_data[, predictors] )
```

