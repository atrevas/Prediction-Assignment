---
title: "Practical Data Science - Prediction Assignment"
output: html_document
---

## Introduction

This report describes the results of Practical Machine Learning Prediction Project. The data set we will use is from accelerometers on the belt, forearm, arm and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 diferent ways. 
The goal is to use the provided training cases to build a model to predict the manner in which 20 test cases did the exercise. 

## Load necessary packages
```{r, message=FALSE, warning=FALSE}
library(dplyr)      # For data manipulation functions
library(caret)      # For classification and regression training
library(rpart.plot) # For the fancyRpartPlot function
library(rattle)     # For prettier tree plots
library(ggplot2)    # For graphics
```


## Create function to load data
```{r}
load_data <- function(file){
  # Build a path for the data file name
  fn <- file.path('data', file)
  
  df <- read.csv(fn, stringsAsFactor = TRUE, na.strings = c('NA', '')
                  , quote = '\"')
  return (df)
}
```

## Load the data files
```{r, cache=TRUE}
###############################################################################
# Load the data
###############################################################################
# Load the training data
raw_train_data <- load_data('pml-training.csv')
raw_train_data <- tbl_df(raw_train_data)

# Load the test data
raw_test_data <- load_data('pml-testing.csv')
raw_test_data <- tbl_df(raw_test_data)
```

## Take a first look at the raw data
```{r}
###############################################################################
# First look at the raw data
###############################################################################
# How many observation and variables we have
d <- dim(raw_train_data)
df1 <- data.frame(Data = 'train',Obs = d[1], Vars = d[2])
d <- dim(raw_test_data)
df2 <- data.frame(Data = 'test',Obs = d[1], Vars = d[2])
rbind(df1, df2)

glimpse(raw_train_data)
```

There are `r df1$Obs` observations and `r df1$Vars` variables in the training data set. The response variable is *classe*.

## Look for variables with lots of NAs and remove them

After a quick look at the data, we realize that there are some variables with lots of *NAs* values. So in order to simplify our job, we wil remove the variables with a high percentage of *NAs* values.

```{r}
###############################################################################
# Remove variables with lots of NAs
###############################################################################
# Get the percentage of NAs values in each variable
perc_nas <- sapply(raw_train_data, function (x) sum(is.na(x)) / length(x))

# Get a list of variables with more than 90% of NAs values
high_nas <- names(perc_nas[perc_nas > 0.9])

# Remove the hight NAs variables from train data
train_data <- raw_train_data %>%
  select( - one_of(high_nas))

# Remove the hight NAs variables from test data
test_data <- raw_test_data %>%
  select( - one_of(high_nas))
```

## Remove timestamp variables

We will remove the timestamp variables because we believe they do not add to the model.

```{r}
###############################################################################
# Remove timestamp variables
###############################################################################
# Create a list of variables to remove
var_list <- c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')

# Remove from train data
train_data <- train_data %>%
   select(-one_of(var_list))

# Remove from test data
test_data <- test_data %>%
   select(-one_of(var_list))
```

## Remove zero and near zero-variance variables

We identify the predictors that have only a few unique values and remove them. This kind of predictors may cause problems depending on the type of model chosen.

```{r}
###############################################################################
# Remove zero and near zero-variance variables
###############################################################################
nzv <- nearZeroVar(train_data)

# Summarize near zero variance variables
summary(train_data[ , nzv])

# Remove from the train data
train_data <- train_data[ , -nzv]

# Remove from the test data
test_data <- test_data[ , -nzv]
```

## Create a list of names of predictors variables
```{r}
###############################################################################
# Create list of names of predictors variables
###############################################################################
# Create list of numeric variables 
is_num <- sapply(train_data, is.numeric)
numerics <- names(is_num[is_num == TRUE])

# Remove the X variable as it is a kind of key that is unique for each
# observation
predictors <- numerics[numerics != 'X']

```

## Let us start with a Tree model
```{r}
###############################################################################
# Build a tree model
###############################################################################
set.seed(123)

control <- trainControl(method = 'cv'   # Cross-validation
                        , number = 5   # Number of subsamples to take
                        )

tree_model <- train(
                    x = train_data[, predictors] # Predictors
                    , y = train_data$classe      # Outcome variable
                    , method = 'rpart'
                    , trControl = control
                    , tuneLength = 10 # Number of models to be evaluated
                    , metric = 'Accuracy'
                    )

# Get the accuracy from the resampling results for the chosen model
acc <-getTrainPerf(tree_model)[1]

# Plot the tree
fancyRpartPlot(tree_model$finalModel, main = '', sub = '')

# Build a data frame with a ranking of variables importance in the tree model
vi <- varImp(tree_model)$importance
vi_df <- data.frame( var = rownames(vi), imp = vi$Overall)

# Plot the ranking of variables importance
vi_df %>% 
  top_n(10, imp) %>% 
  arrange(desc(imp)) %>%
  ggplot(aes(x = imp, y = reorder(var, imp))) +
    geom_point(size = 3)
```

### Estimation of the out of sample error

From the results above, we estimate that the out of sample error is: `r sprintf('%.2f%%', (1 - acc) * 100)`, which is the average percentage of observation that were incorrectly classified in the cross validation process.
  
## Plot the variable at the root of the tree  
The variable at the root of tree is the one that best separates the outcomes.

```{r}
train_data %>%
  ggplot(aes(x = roll_belt , colour = classe)) +
    geom_line(stat = 'density')

```

## 
