---
title: "Practical Data Science - Prediction Assignment"
output: html_document
---

## Introduction

In this project, we will use a data set from accelerometers on the belt, forearm, arm and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 diferent ways. 
The goal is to use the provided training cases to build a model to predict the manner in which 20 test cases did the exercise. 

## Load necessary packages
```{r, message=FALSE, warning=FALSE}
library(caret)        # For machine learning functions
library(dplyr)        # For data manipulation functions
library(ggplot2)      # For graphics
library(randomForest) # For random forests
```

## Data sources

The data sets came from the following urls.

### Training data

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

### Testing data

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

## Load the data

### Create a function to load data
```{r}
load_data <- function(file){
  # Build a path for the data file name
  fn <- file.path('data', file)
  
  df <- read.csv(fn, stringsAsFactor = TRUE, na.strings = c('NA', '')
                  , quote = '\"')
  return (df)
}
```

### Load the data files
```{r, cache=TRUE}
# Load the training data
raw_train_data <- load_data('pml-training.csv')
raw_train_data <- tbl_df(raw_train_data)

# Load the test data
raw_test_data <- load_data('pml-testing.csv')
raw_test_data <- tbl_df(raw_test_data)
```

## Take a first look at the raw data

### Print number of observations and variables
```{r}
# How many observation and variables we have
d <- dim(raw_train_data)
df1 <- data.frame(Data = 'train',Obs = d[1], Vars = d[2])
d <- dim(raw_test_data)
df2 <- data.frame(Data = 'test',Obs = d[1], Vars = d[2])
rbind(df1, df2)
```

There are `r df1$Obs` observations and `r df1$Vars` variables in the training data set. The response variable is *classe*.

### Print a summary of the training data
```{r}
glimpse(raw_train_data)
```

## Data Preparation

### Look for variables with lots of NAs and remove them

After a quick look at the data, we realize that there are some variables with lots of *NAs* values. So in order to simplify our job, we wil remove the variables with a high percentage of *NAs* values.

```{r}
# Get the percentage of NAs values in each variable
perc_nas <- sapply(raw_train_data, function (x) sum(is.na(x)) / length(x))

# Get a list of variables with more than 90% of NAs values
high_nas <- names(perc_nas[perc_nas > 0.9])

# Remove the hight NAs variables from train data
train_data <- raw_train_data %>%
  select( - one_of(high_nas))

# Remove the hight NAs variables from test data
test_data <- raw_test_data %>%
  select( - one_of(high_nas))
```

### Remove variables that do not add to the model

We will remove the variables we believe do not add to the model.

```{r}
# Create a list of variables to remove
var_list <- c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp'
              , 'X', 'user_name')

# Remove from train data
train_data <- train_data %>%
   select(-one_of(var_list))

# Remove from test data
test_data <- test_data %>%
   select(-one_of(var_list))
```

### Remove zero and near-zero variance variables

We identify the predictors that have near-zero variance and remove them. This kind of predictors may cause problems depending on the type of model chosen.

```{r}
# Get a list of variables with near-zero variance
nzv <- nearZeroVar(train_data)

# Summarize near zero variance variables
summary(train_data[ , nzv])

# Remove from the train data
train_data <- train_data[ , -nzv]

# Remove from the test data
test_data <- test_data[ , -nzv]
```

### Data partition

We will split the original training data in order to have two data slices: one to train the model and another to validate the model. Besides, using the second slice to predict the response variable, we will be able to estimate the out-of-sample error rate.

```{r}
# Set the seed to generate the same random numbers always.
set.seed(123)

ind <- createDataPartition(y = train_data$classe
                           , p = 0.6
                           , list = FALSE)

train_slice <- train_data[ind, ]
valid_slice <- train_data[-ind, ]

dim(train_slice)
dim(valid_slice)
```

## Model building

We will build our model on the train data slice. As random forests are considered one of the top performing algorithms, we decided to stick to it.
 
### Define training parameters

By default, the train function of the Caret package uses the bootstrap for resampling. We will switch to k-fold cross-validation instead.

```{r}
# Generate parameters to be used by the train function
control <- trainControl(method = 'cv' # K-fold cross-validation
                        , number = 2  # Number of folds to use
                        )
```

### Training

Train a random forests model, using the train data slice and the parameters defined above. The *tuneLength* parameter is used to specify the number of models to evaluate. We decided to set this parameter to a minimum, because random forests models may take some time to train. 

#### Train the model
```{r}
# Start the clock
ptm <- proc.time()

rf_model <- train (
                    classe ~ .            # Classe is the response variable
                    , data = train_slice  # Use the trian data slice
                    , method = 'rf'       # Random forests
                    , trControl = control
                    , tuneLength = 1      # Number of models to be evaluated
                    , metric = 'Accuracy'
                    )

# Stop the clock
ptm <- proc.time() - ptm

```

Elapsed training time: `r sprintf('%.2f seconds',as.numeric(ptm['elapsed']))`

#### Print the training results
```{r}
# Print the training results
rf_model
```

#### Print the final model
```{r}
rf_model$finalModel
```

## Model Validation

Now that we have our model built, we will validate it on the validation data slice and estimate our out-of-sample error rate.

### Predict on the validation data
```{r}
# Predict the response variable for the validation data set
pred <- predict(rf_model, valid_slice %>% select(-classe))

# Save the confusion matrix
cm <- confusionMatrix(pred, valid_slice$classe)

# Display the confusion matrix
cm

# Get the overall accuracy from the confusion matrix
acc <- as.numeric(cm$overall['Accuracy'])

# Get the estimated error rate
err <- 1 - acc
```

### Estimation of the out of sample error

From the results above, the estimated accuracy of the model is `r sprintf('%.2f%%',acc * 100)` and the corresponding estimated out-of-sample error rate is `r sprintf('%.2f%%', err * 100)`.

## Final prediction

The error rate estimated from the random forests model seems great, so we decide to apply this very model to the 20 test cases provided for final prediction evaluation.

```{r}
pred <- predict(rf_model, test_data %>% select(-problem_id))
```

